# Knit together results????

load('./runs/____')
LHS.trial <- LHS.simple
results.trial <- results.simple

# Show results for cobweb
#Plot findings: 
#1. empirical cumulative distribution function used to illustrate the distribution of the model results
plotecdf(LHS.trial) #outcome has a high probability in the steepest parts of the graph 

#2. scatterplot of the result as a function of each parameter: distribution of values returned by the model 
# in the parameter space sampled by the hypercube and how sensible are these model responses to 
# the variation of each parameter.
plotscatter(LHS.trial)

#3. partial correlation coefficient measures how strong are the inear associations between the result 
# and each input parameter, after removing the linear effect of the other parameters. 
# (CI generated by bootstrapping)
plotprcc(LHS.trial)

pic(LHS.trial, nboot=40) #pic (partial inclination coefficient) is the sensitivity" of the model response 
# in respect to each parameter
#represent the beta terms in y = alpha + beta*x regressions, 
#after removing the linear effect of the other parameters

# 4. Cobweb
outcome.df<-as.data.frame(cbind(LHS.trial$data,results.trial)) #dummy matrix with parameter values in columns and outcome in last column
names(outcome.df)<- c(parameters_simple,'outcome') #name the columns of the dummy matrix 
for (i in 1:nrow(outcome.df)) {       #label the rows of parameter values that produced top 5% of the outcomes
  if (outcome.df$outcome[i]<quantile(outcome.df$outcome,probs = 0.95)) { 
    outcome.df$top5[i] <-0 } else {
      outcome.df$top5[i] <-1
    }
}
require(plotrix) #load MASS package
blue<-alpha("lightskyblue1", alpha=0.3)
red<-alpha("red", alpha=0.6)
colors<- c(blue, red) #choose 2 colors - 1 for parameters that produced top 5% of outcomes and one for the rest
outcome.df$top5<- as.factor(outcome.df$top5)
parcoordlabel<-function (x, col = 1, lty = 1,  lblcol="black",...) 
{
  df <- as.data.frame(x)
  pr <- lapply(df, pretty)
  rx <- lapply(pr, range, na.rm = TRUE)
  x <- mapply(function(x,r) {
    (x-r[1])/(r[2]-r[1])
  },
  df, rx)
  matplot(1L:ncol(x), t(x), type = "l", col = col, lty = lty, 
          xlab = "", ylab = "", axes = FALSE,...)
  axis(1, at = 1L:ncol(x), labels = c(colnames(x)), las = 2)
  for (i in 1L:ncol(x)) {
    lines(c(i, i), c(0, 1), col = "grey")
    text(c(i, i), seq(0,1,length.out=length(pr[[i]])), labels = pr[[i]], 
         xpd = NA, col=lblcol, cex=0.5)
  }
  invisible()
}
parcoordlabel(outcome.df[,c(1:length(factors))], col = colors[outcome.df$top5])

#5. Check agreement between runs to decide if our sample size for adequate 
# Symmetric Blest Measure of Agreement (SBMA) between the PRCC coeffients of two runs with different sample sizes.
#check.LHS.trial <- LHS(modelRun.trial, factors.trial, 250, q.trial, q.arg.trial, cl=cl)
(mySbma <- sbma(LHS.trial, check.LHS.trial))
# value of -1 indicates complete disagreement between the runs 
# value of 1 indicated complete agreement  (>0.7 acceptable)
#caveat: if none of the model parameters is monotonically correlated with the output, 
#the agreement between runs may stay as low as 0.2 even for very large hypercubes.
